---
title: "Client Report - Project 5"
subtitle: "Course DS 250"
author: "Luis Fernando Mendoza"
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
execute: 
  warning: false
    
---

```{python}
#| label: libraries
#| include: false
import pandas as pd
import numpy as np
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

```


## Elevator pitch
Use the force to predict the current income of your job's candidates!
Stop being in the dark side! Come and see how this study can help you indentify top talent, true jedys for you company.
Our predictive model allows you to streamline your recruitment efforts and make more informed hiring decisions. 
Join us on a journey to bring the power of data science to the galaxy of recruitment.

```{python}
#| label: project data
#| code-summary: Read and format project data
# Include and execute your code here
df = pd.read_csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/star-wars-survey/StarWars.csv", encoding="ISO-8859-1")
```

__Highlight the Questions and Tasks__

## Question|Task 1

Shorten the column names and clean them up for easier use with pandas. Provide a table or list that exemplifies how you fixed the names.

Here you can see an example of the result of the surveys where we asked people about which star wards movies they have seen and their salaries. 

```{python}
# Define the new column names
new_column_names = [
    "respondent_id",
    "seen_star_wars",
    "star_wars_fan",
    "seen_sw_ep1", "seen_sw_ep2", "seen_sw_ep3", "seen_sw_ep4", "seen_sw_ep5", "seen_sw_ep6",
    "ranking_sw_ep1", "ranking_sw_ep2", "ranking_sw_ep3", "ranking_sw_ep4", "ranking_sw_ep5", "ranking_sw_ep6",
    "fav_character_han", "fav_character_luke", "fav_character_leia", "fav_character_anakin",
    "fav_character_obi_wan", "fav_character_emperor", "fav_character_darth_vader", "fav_character_lando",
    "fav_character_boba_fett", "fav_character_c_3p0", "fav_character_r2_d2", "fav_character_jar_jar",
    "fav_character_padme", "fav_character_yoda",
    "shot_first",
    "expanded_universe",
    "eu_fan",
    "star_trek_fan",
    "gender",
    "age_midpoint",
    "income",
    "education",
    "location"
]

# Assign new column names to the DataFrame
df.columns = new_column_names

# Display the modified DataFrame
df.head(10)


```

__Highlight the Questions and Tasks__

## Question|Task 2

Clean and format the data so that it can be used in a machine learning model. As you format the data, you should complete each item listed below. In your final report provide example(s) of the reformatted data with a short description of the changes made.

a-Filter the dataset to respondents that have seen at least one film.

```{python}
# Filter the dataset to respondents that have seen at least one film
df_filtered = df[df["seen_star_wars"] == "Yes"].copy()

```

b- Create a new column that converts the age ranges to a single number. Drop the age range categorical column.

```{python}
# Convert age ranges to a single number
age_mapping = {
    "18-29": 24,
    "30-44": 37,
    "45-60": 52.5,
    "> 60": 70
}
df_filtered["age_midpoint"] = df_filtered["age_midpoint"].map(age_mapping)

# Drop the original age range column
df_filtered.drop("age_midpoint", axis=1, inplace=True)

```

c- Create a new column that converts the education groupings to a single number. Drop the school categorical column

```{python}
# Convert education groupings to a single number
education_mapping = {
    "High school degree": 1,
    "Some college or Associate degree": 2,
    "Bachelor degree": 3,
    "Graduate degree": 4
}
df_filtered["education_level"] = df_filtered["education"].map(education_mapping)
# Drop the original education column
df_filtered.drop("education", axis=1, inplace=True)

```

d- Create a new column that converts the income ranges to a single number. Drop the income range categorical column.

```{python}
# Convert income ranges to a single number
income_mapping = {
    "$0 - $24,999": 12500,
    "$25,000 - $49,999": 37500,
    "$50,000 - $99,999": 75000,
    "$100,000 - $149,999": 125000,
    "$150,000+": 175000
}
df_filtered["income_midpoint"] = df_filtered["income"].map(income_mapping)
# Drop the original income range column
df_filtered.drop("income", axis=1, inplace=True)

```

e- Create your target (also known as “y” or “label”) column based on the new income range column.

```{python}
# Create the target column
df_filtered["target"] = (df_filtered["income_midpoint"] >= 50000).astype(int)

```

f- One-hot encode all remaining categorical columns.

```{python}
# One-hot encode remaining categorical columns
df_encoded = pd.get_dummies(df_filtered, columns=["gender", "location", "star_trek_fan", "expanded_universe"])

```

__Highlight the Questions and Tasks__

## Question|Task 3

Validate that the data provided on GitHub lines up with the article by recreating 2 of the visuals from the article.

```{python}
# Define a dictionary mapping column names to movie titles
movie_titles = {
    "seen_sw_ep1": "The Phantom Menace",
    "seen_sw_ep2": "Attack of the Clones",
    "seen_sw_ep3": "Revenge of the Sith",
    "seen_sw_ep4": "A New Hope",
    "seen_sw_ep5": "The Empire Strikes Back",
    "seen_sw_ep6": "Return of the Jedi"
}

# Convert movie seen columns to binary values
for col in df_filtered.columns[3:9]:
    df_filtered[col] = df_filtered[col].notnull().astype(int)

# Now, you can calculate the sum of each column
seen_counts = df_filtered.iloc[:, 3:9].sum()

# Calculate the total number of respondents
total_respondents = len(df_filtered)

# Convert counts to percentages
seen_percentages = (seen_counts / total_respondents) * 100

# Replace column names with movie titles
seen_percentages.index = [movie_titles[col] for col in seen_percentages.index]

# Create a bar plot
fig1 = px.bar(x=seen_percentages.values, y=seen_percentages.index, 
              labels={'x': 'Percentage of Respondents (%)', 'y': 'Star Wars Movies'},
              title='Which Star Wars Movies Have You Seen?', orientation='h')
fig1.show()


```


```{python}
# Convert the columns containing movie rankings to numeric
df_filtered.iloc[:, 9:15] = df_filtered.iloc[:, 9:15].astype(float)

# Calculate the average rankings of Star Wars movies
avg_rankings = df_filtered.iloc[:, 9:15].mean()

# Define a dictionary mapping column names to movie titles
movie_titles = {
    "ranking_sw_ep1": "The Phantom Menace",
    "ranking_sw_ep2": "Attack of the Clones",
    "ranking_sw_ep3": "Revenge of the Sith",
    "ranking_sw_ep4": "A New Hope",
    "ranking_sw_ep5": "The Empire Strikes Back",
    "ranking_sw_ep6": "Return of the Jedi"
}

# Create a bar plot with movie titles
fig2 = px.bar(y=avg_rankings.values, x=[movie_titles[col] for col in avg_rankings.index], 
              labels={'y': 'Average Ranking', 'x': 'Star Wars Movies'},
              title='What is the best Star Wars Movie?', orientation='h')
fig2.show()

```

__Highlight the Questions and Tasks__

## Question|Task 4


Build a machine learning model that predicts whether a person makes more than $50k. Describe your model and report the accuracy.


```{python}
# One-hot encode categorical columns
df_encoded = pd.get_dummies(df_filtered, columns=["gender", "location", "star_trek_fan", "expanded_universe"])

# Define features (X) and target variable (y)
X = df_encoded.drop("target", axis=1)
y = df_encoded["target"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Get list of categorical columns
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()

# Initialize the imputer with mean strategy, excluding categorical columns
imputer = SimpleImputer(strategy='mean')
X_train_numeric = X_train.drop(columns=categorical_cols)
X_test_numeric = X_test.drop(columns=categorical_cols)

# Fit the imputer on the training data and transform the training data
X_train_imputed = imputer.fit_transform(X_train_numeric)

# Transform the testing data using the fitted imputer
X_test_imputed = imputer.transform(X_test_numeric)


```


```{python}
# Initialize and train the Random Forest Classifier
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train_imputed, y_train)

# Predict the target variable on the testing set
y_pred = rf_classifier.predict(X_test_imputed)

# Predict the target variable on the testing set
y_pred = rf_classifier.predict(X_test_imputed)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Calculate precision
precision = precision_score(y_test, y_pred)
print("Precision:", precision)

# Calculate recall
recall = recall_score(y_test, y_pred)
print("Recall:", recall)

# Calculate F1-score
f1 = f1_score(y_test, y_pred)
print("F1-score:", f1)
```


```{python}
# Calculate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

```


```{python}
```


```{python}
# Define hyperparameters grid
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize Random Forest Classifier
rf_classifier_tuned = RandomForestClassifier(random_state=42)

# Perform Grid Search Cross Validation
grid_search = GridSearchCV(rf_classifier_tuned, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_imputed, y_train)

# Print best parameters
print("Best Parameters:", grid_search.best_params_)

# Get best estimator
best_rf_classifier = grid_search.best_estimator_

# Predict the target variable on the testing set with the best estimator
y_pred_tuned = best_rf_classifier.predict(X_test_imputed)

```

